{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import  Sequential \n",
    "from tensorflow.keras.layers import Dense,Embedding,GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>605</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2349</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3624</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4685</td>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2030</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 label                                               text  \\\n",
       "0         605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
       "1        2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
       "2        3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
       "3        4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
       "4        2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
       "\n",
       "   label_num  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          1  \n",
       "4          0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('spam_ham_dataset.csv')\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'label', 'text', 'label_num'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "label         0\n",
       "text          0\n",
       "label_num     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset['text']\n",
    "y = dataset['label_num']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_seq_length = max([len(seq) for seq in X_train_seq])\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_seq_length, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_seq_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer='adam', units=64, activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=64, input_length=max_seq_length))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(units, activation=activation))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91999\\AppData\\Local\\Temp\\ipykernel_14360\\2566461152.py:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=create_model, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=create_model, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "259/259 [==============================] - 12s 42ms/step - loss: 0.6106 - accuracy: 0.7065 - val_loss: 0.5908 - val_accuracy: 0.7169\n",
      "Epoch 2/100\n",
      "259/259 [==============================] - 11s 41ms/step - loss: 0.5933 - accuracy: 0.7084 - val_loss: 0.5721 - val_accuracy: 0.7169\n",
      "Epoch 3/100\n",
      "259/259 [==============================] - 11s 41ms/step - loss: 0.5389 - accuracy: 0.7234 - val_loss: 0.4676 - val_accuracy: 0.7430\n",
      "Epoch 4/100\n",
      "259/259 [==============================] - 11s 43ms/step - loss: 0.3910 - accuracy: 0.8194 - val_loss: 0.3100 - val_accuracy: 0.8696\n",
      "Epoch 5/100\n",
      "259/259 [==============================] - 11s 41ms/step - loss: 0.2600 - accuracy: 0.9134 - val_loss: 0.2141 - val_accuracy: 0.9304\n",
      "Epoch 6/100\n",
      "259/259 [==============================] - 11s 43ms/step - loss: 0.1809 - accuracy: 0.9512 - val_loss: 0.1740 - val_accuracy: 0.9188\n",
      "Epoch 7/100\n",
      "259/259 [==============================] - 11s 42ms/step - loss: 0.1326 - accuracy: 0.9727 - val_loss: 0.1243 - val_accuracy: 0.9778\n",
      "Epoch 8/100\n",
      "259/259 [==============================] - 11s 44ms/step - loss: 0.1076 - accuracy: 0.9724 - val_loss: 0.1107 - val_accuracy: 0.9681\n",
      "Epoch 9/100\n",
      "259/259 [==============================] - 11s 42ms/step - loss: 0.0955 - accuracy: 0.9707 - val_loss: 0.1110 - val_accuracy: 0.9556\n",
      "Epoch 10/100\n",
      "259/259 [==============================] - 11s 43ms/step - loss: 0.0753 - accuracy: 0.9804 - val_loss: 0.0867 - val_accuracy: 0.9826\n",
      "Epoch 11/100\n",
      "259/259 [==============================] - 11s 42ms/step - loss: 0.0635 - accuracy: 0.9833 - val_loss: 0.0824 - val_accuracy: 0.9778\n",
      "Epoch 12/100\n",
      "259/259 [==============================] - 11s 43ms/step - loss: 0.0576 - accuracy: 0.9843 - val_loss: 0.0765 - val_accuracy: 0.9691\n",
      "Epoch 13/100\n",
      "259/259 [==============================] - 11s 43ms/step - loss: 0.0519 - accuracy: 0.9855 - val_loss: 0.0676 - val_accuracy: 0.9826\n",
      "Epoch 14/100\n",
      "259/259 [==============================] - 11s 42ms/step - loss: 0.0487 - accuracy: 0.9860 - val_loss: 0.0663 - val_accuracy: 0.9826\n",
      "Epoch 15/100\n",
      "259/259 [==============================] - 10s 40ms/step - loss: 0.0426 - accuracy: 0.9872 - val_loss: 0.0634 - val_accuracy: 0.9768\n",
      "Epoch 16/100\n",
      "259/259 [==============================] - 11s 41ms/step - loss: 0.0391 - accuracy: 0.9894 - val_loss: 0.0592 - val_accuracy: 0.9826\n",
      "Epoch 17/100\n",
      "259/259 [==============================] - 10s 40ms/step - loss: 0.0368 - accuracy: 0.9882 - val_loss: 0.0583 - val_accuracy: 0.9845\n",
      "Epoch 18/100\n",
      "259/259 [==============================] - 10s 40ms/step - loss: 0.0398 - accuracy: 0.9860 - val_loss: 0.0575 - val_accuracy: 0.9845\n",
      "Epoch 19/100\n",
      "259/259 [==============================] - 10s 40ms/step - loss: 0.0299 - accuracy: 0.9894 - val_loss: 0.0743 - val_accuracy: 0.9729\n",
      "Epoch 20/100\n",
      "259/259 [==============================] - 11s 42ms/step - loss: 0.0342 - accuracy: 0.9845 - val_loss: 0.0582 - val_accuracy: 0.9826\n",
      "Epoch 21/100\n",
      "259/259 [==============================] - 11s 42ms/step - loss: 0.0388 - accuracy: 0.9843 - val_loss: 0.0563 - val_accuracy: 0.9816\n",
      "Epoch 22/100\n",
      "259/259 [==============================] - 11s 41ms/step - loss: 0.0253 - accuracy: 0.9920 - val_loss: 0.0677 - val_accuracy: 0.9729\n",
      "Epoch 23/100\n",
      "259/259 [==============================] - 10s 40ms/step - loss: 0.0237 - accuracy: 0.9923 - val_loss: 0.1592 - val_accuracy: 0.9285\n",
      "Epoch 24/100\n",
      "259/259 [==============================] - 10s 39ms/step - loss: 0.0271 - accuracy: 0.9903 - val_loss: 0.0600 - val_accuracy: 0.9855\n",
      "Epoch 25/100\n",
      "259/259 [==============================] - 10s 40ms/step - loss: 0.0283 - accuracy: 0.9903 - val_loss: 0.0687 - val_accuracy: 0.9768\n",
      "Epoch 26/100\n",
      "259/259 [==============================] - 10s 39ms/step - loss: 0.0223 - accuracy: 0.9920 - val_loss: 0.1418 - val_accuracy: 0.9440\n",
      "Epoch 27/100\n",
      "259/259 [==============================] - 10s 38ms/step - loss: 0.0238 - accuracy: 0.9911 - val_loss: 0.0550 - val_accuracy: 0.9816\n",
      "Epoch 28/100\n",
      "259/259 [==============================] - 10s 38ms/step - loss: 0.0203 - accuracy: 0.9930 - val_loss: 0.0566 - val_accuracy: 0.9807\n",
      "Epoch 29/100\n",
      "259/259 [==============================] - 10s 40ms/step - loss: 0.0215 - accuracy: 0.9925 - val_loss: 0.0655 - val_accuracy: 0.9768\n",
      "Epoch 30/100\n",
      "259/259 [==============================] - 10s 40ms/step - loss: 0.0185 - accuracy: 0.9927 - val_loss: 0.0610 - val_accuracy: 0.9787\n",
      "Epoch 31/100\n",
      "259/259 [==============================] - 10s 39ms/step - loss: 0.0197 - accuracy: 0.9923 - val_loss: 0.0838 - val_accuracy: 0.9720\n",
      "Epoch 32/100\n",
      "259/259 [==============================] - 10s 39ms/step - loss: 0.0192 - accuracy: 0.9937 - val_loss: 0.1569 - val_accuracy: 0.9353\n",
      "Epoch 33/100\n",
      "259/259 [==============================] - 10s 39ms/step - loss: 0.0192 - accuracy: 0.9940 - val_loss: 0.0592 - val_accuracy: 0.9787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20e952fe3a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=64, input_length=max_seq_length))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', \n",
    "    patience=3\n",
    ")\n",
    "\n",
    "model.fit(X_train_pad, y_train, validation_data=(X_test_pad, y_test), epochs=100, batch_size=16, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 1s 20ms/step - loss: 0.0592 - accuracy: 0.9787\n",
      "loss : 0.0592464916408062\n",
      "accuracy : 0.9787439703941345\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test_pad, y_test)\n",
    "print(f\"loss : {loss}\")\n",
    "print(f\"accuracy : {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 1s 20ms/step\n",
      "confusion matrics:\n",
      "[[726  16]\n",
      " [  6 287]]\n",
      "True Positives (TP) : 287\n",
      "False Positives (TN) : 726\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred_prob = model.predict(X_test_pad)\n",
    "\n",
    "y_pred = np.round(y_pred_prob).astype(int)\n",
    "\n",
    "# Calculer la matrice de confusion\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Extraire les valeurs TP et TN de la matrice de confusion\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "print(\"confusion matrics:\")\n",
    "print(conf_matrix)\n",
    "print(\"True Positives (TP) :\", tp)\n",
    "print(\"False Positives (TN) :\", tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       742\n",
      "           1       0.95      0.98      0.96       293\n",
      "\n",
      "    accuracy                           0.98      1035\n",
      "   macro avg       0.97      0.98      0.97      1035\n",
      "weighted avg       0.98      0.98      0.98      1035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"classification report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
